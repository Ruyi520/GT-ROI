{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Tracking with ROI\n",
    "\n",
    "This module implements an attention tracking system using state of the art face detection algorithms (OpenCV and Dlib shape predictor class). This notebook requires proper installation  these library dependencies.\n",
    "\n",
    "The is sectioned into:\n",
    "\n",
    "* Head pose CNN Classifier (using a pretrained VGG16 model)\n",
    "* Physical environment parameters\n",
    "\n",
    "* Face detector: Dlib face shape detector\n",
    "* Eye detector: iris tracking reference()\n",
    "\n",
    "The above functions are exposed to our simulating environment which we refer to in the module named ().\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import imutils.video\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import time\n",
    "import numpy.linalg as LA\n",
    "import os, os.path, sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train_data = []\n",
    "data = \"../data/clean_data/\"\n",
    "train_dir = \"../data/tf_data/\"\n",
    "clean_dups = r\"[A-Z]+_[0-9]+\"\n",
    "count = 0 \n",
    "\n",
    "for i in os.listdir(train_dir):\n",
    "    m = re.match(clean_dups, i)\n",
    "    if m is not None:\n",
    "        match = m.group(0)\n",
    "        print(match)\n",
    "        if i not in train_data and match != None:\n",
    "            train_data.append(i)\n",
    "            img = cv2.imread(train_dir+i)\n",
    "            cv2.imwrite(os.path.join(data,str(count+1)+\".jpg\"), img)\n",
    "            count+=1\n",
    "            if count ==50 : break\n",
    "    \n",
    "size = len(os.listdir(data))\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a spiral function that returns an array of tuple pairs\n",
    "# used for generating our mouse position (x,y) values\n",
    "def spiral(X, Y):\n",
    "    pairs = []\n",
    "    x = y = 0\n",
    "    dx = 0\n",
    "    dy = -5\n",
    "    for i in range(max(X, Y)**2):\n",
    "        if (-X/2 < x <= X/2) and (-Y/2 < y <= Y/2):\n",
    "            pairs.append((x, y))\n",
    "        if x == y or (x < 0 and x == -y) or (x > 0 and x == 5-y):\n",
    "            dx, dy = -dy, dx\n",
    "        x, y = x+dx, y+dy\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-50c854e70475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame, sys, math\n",
    "\n",
    "#Here, we initialize a pygame environment to compute parameters for our game loop.\n",
    "\n",
    "pygame.init()\n",
    "W,H = 800, 800;\n",
    "x, y = W//2, H//2\n",
    "radius = 40\n",
    "color = (255,0,0)\n",
    "\n",
    "points = spiral(W, H)\n",
    "screen = pygame.display.set_mode((W, H))\n",
    "count = 0\n",
    "run = True\n",
    "point = iter(points)\n",
    "\n",
    "while run :\n",
    "    #dt = clock.tick()/100\n",
    "    pygame.time.delay(3)\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT: run = False\n",
    "        #if event.type == MOUSEBUTTONUP: None\n",
    "    screen.fill((0,0,0))\n",
    "    pygame.draw.circle(screen, color, (x, y), radius)\n",
    "    \n",
    "    x, y = next(point)\n",
    "    x, y = x+W//2, y+H//2\n",
    "    \n",
    "    pygame.display.update()\n",
    "    if count == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "session = boto3.Session(profile_name='default')                        \n",
    "client = session.client('rekognition')\n",
    "\n",
    "W, H = 450,450\n",
    "\n",
    "def detect_labels_local_file(path):\n",
    "    # @params: input - path containing all face images\n",
    "    # @params: output- This function returns the a list of tuples holding the roll, \n",
    "    #                  pitch and yaw values of face images in the input path.\n",
    "    \n",
    "    label_arr = []\n",
    "    \n",
    "    #iterate over n images in input path\n",
    "    for i in os.listdir(path):\n",
    "        if i.endswith('.jpg'):\n",
    "            old_photo = os.path.join(path,i)\n",
    "            with open(old_photo, 'rb') as image:\n",
    "                \n",
    "                #initiate our aws API service for each image\n",
    "                response = client.detect_faces(Image={'Bytes': image.read()})\n",
    "                label = response['FaceDetails']\n",
    "                \n",
    "                #check for a single face in each image\n",
    "                if len(label) == 1:\n",
    "                    for label in label:\n",
    "                        pose_labels = (int(label['Pose']['Roll']),\n",
    "                                       int(label['Pose']['Yaw']),\n",
    "                                       int(label['Pose']['Pitch']))\n",
    "                        \n",
    "                        x1, y1, w, h = (int(label['BoundingBox']['Left']*W), \n",
    "                                        int(label['BoundingBox']['Top']*H), \n",
    "                                        int(label['BoundingBox']['Width']*W),\n",
    "                                        int(label['BoundingBox']['Height']*H))    \n",
    "                        \n",
    "                        #convert boundingbox format\n",
    "                        x1, y1, x2, y2 = x1, y1, (x1+w), (y1+h)\n",
    "                        label_arr.extend([i, pose_labels])\n",
    "                        cropped_photo = cv2.imread(old_photo)\n",
    "                        \n",
    "                        #using our boundingbox, crop face of each image\n",
    "                        new_photo = cropped_photo[y:y2, x:x2]\n",
    "                        \n",
    "                        #save our images into the same path as our input images\n",
    "                        cv2.imwrite(old_photo, new_photo)\n",
    "                        print(pose_labels)\n",
    "                else:\n",
    "                    os.remove(old_photo)\n",
    "    return label_arr\n",
    "print(detect_labels_local_file(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGmodel = keras.applications.VGG16(input_shape=(224, 224, 3),\n",
    "                                 include_top=False,\n",
    "                                 weights='imagenet')\n",
    "VGGmodel.trainable = False\n",
    "\n",
    "def create_model(model_type):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D()4\n",
    "    \n",
    "    #model.add(Reshape(()))\n",
    "    model.add(Convolution2D(64, kernel_size=(1,1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Convolution2D(128, kernel_size=(1,1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if model_type == \"head_pose_model\":\n",
    "        model.add(Dense(3))\n",
    "        pass\n",
    "    elif model_type == \"eye_gaze_model\":\n",
    "        model.add(Dense(2))\n",
    "    \n",
    "    return model\n",
    "\n",
    "hp_model = create_model('head_pose_model')\n",
    "hp_model = Sequential([VGGmodel,hp_model])\n",
    "\n",
    "#eg_model = Sequential([VGGmodel,eg_model])\n",
    "hp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "epochs = 20\n",
    "size = len(train_data)\n",
    "\n",
    "labels = pd.read_csv('../data/tf_label/labels.csv')\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "valid_gen = ImageDataGenerator(resclae=1./255)\n",
    "\n",
    "train_gen = train_gen.flow_from_directory(batch_size=batch_size,\n",
    "                                            directory=train_dir,\n",
    "                                            classes=None,\n",
    "                                            shuffle=True,\n",
    "                                            target_size=(img_width, img_width),\n",
    "                                            class_mode='multi_output')\n",
    "def getLabel(x,y):\n",
    "    y = np.array([])\n",
    "    for i in x:\n",
    "        y = y.append(labels[i])\n",
    "    return y\n",
    "    \n",
    "def Generator(train_gen, getLabels):\n",
    "    for x, y in train_gen:\n",
    "        yield x, getLabels(x,y)\n",
    "        \n",
    "gen = Generator(train_gen, getLabels)\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "                    gen, \n",
    "                    batch_size,\n",
    "                    epochs,\n",
    "                    validation_split=0.2,\n",
    "                    steps_per_epoch=(len(train_data)//batch_size))\n",
    "\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our eye aspect ratio below where 0=< ratio =< 1 for closed to opened respectively\n",
    "def Ear(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = LA.norm(eye[1] - eye[5])\n",
    "    B = LA.norm(eye[2] - eye[4])\n",
    "\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = LA.norm(eye[0] - eye[3])\n",
    "\n",
    "    # compute the eye aspect ratio\n",
    "    ratio = (A + B) / (2.0 * C)\n",
    "\n",
    "    # return the eye aspect ratio\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each classroom object is referenced to the World Coordinate of the environment(classroom C)\n",
    "#floor_plane = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((266.61531235321746, 416.782527007985),\n",
       " (array([211., 501.]),\n",
       "  array([342., 502.]),\n",
       "  array([480., 658.]),\n",
       "  array([106., 660.])))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the function below to get the detection plane by \n",
    "# selecting the corners of the \n",
    "\n",
    "import pygame.mouse\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "color = (0,255,0)\n",
    "radius = 3\n",
    "filename = '../data/plane.jpg'\n",
    "scale = 0.2\n",
    "W, H = cv2.imread(filename).shape[:2]\n",
    "W, H = int(W*scale), int(H*scale)\n",
    "\n",
    "def getCorners():\n",
    "    # Get shape of window from size of snapshot save in directory\n",
    "    #W, H = cv2.imread(snapshot).shape[:2]\n",
    "    pygame.init()\n",
    "    points = []\n",
    "    screen = pygame.display.set_mode((H,W))\n",
    "    pygame.display.set_caption('Detect plane\\'s four corners')\n",
    "    background = pygame.image.load(filename).convert()\n",
    "    background = pygame.transform.rotozoom(background,-90,scale)\n",
    "    run = True\n",
    "    screen.blit(background,[0,0])\n",
    "    pygame.display.update()\n",
    "    while run:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:  run = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                pos = pygame.mouse.get_pos()\n",
    "                points.append(pos)\n",
    "                for i in points:\n",
    "                    pygame.draw.circle(screen, color, i, radius)\n",
    "                pygame.display.update()\n",
    "                if len(points) >= 4: run = False\n",
    "    pygame.quit()\n",
    "    exit()\n",
    "    return points\n",
    "\n",
    "def _orderPoints_():\n",
    "    points = np.array(getCorners()).reshape(4,2)\n",
    "    OP = np.zeros(points.shape[:2])\n",
    "\n",
    "    top_left = np.argmin(np.sum(points, axis=1))\n",
    "    top_right = np.argmin(np.diff(points, axis=1))\n",
    "    bottonw_right = np.argmax(np.sum(points, axis=1))\n",
    "    bottonw_left = np.argmax(np.diff(points, axis=1))\n",
    "    \n",
    "    OP[0] = points[top_left]\n",
    "    OP[1] = points[top_right]\n",
    "    OP[2] = points[bottonw_right]\n",
    "    OP[3] = points[bottonw_left]\n",
    "    \n",
    "    \n",
    "    def findIntersection(x1,y1,x2,y2,x3,y3,x4,y4):\n",
    "        px= ( (x1*y2-y1*x2)*(x3-x4)-(x1-x2)*(x3*y4-y3*x4) ) / ( (x1-x2)*(y3-y4)-(y1-y2)*(x3-x4) ) \n",
    "        py= ( (x1*y2-y1*x2)*(y3-y4)-(y1-y2)*(x3*y4-y3*x4) ) / ( (x1-x2)*(y3-y4)-(y1-y2)*(x3-x4) )\n",
    "        return (px, py)\n",
    "    \n",
    "    # We compute Vxy as the vanishing point on the ground plane\n",
    "    Vp = findIntersection(OP[3][0],OP[3][1],OP[0][0],OP[0][1],OP[2][0],OP[2][1],OP[1][0],OP[1][1])\n",
    "    \n",
    "    return Vp, tuple(OP)\n",
    "_orderPoints_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "class Classroom:\n",
    "    __slot__ = ['x', 'y', 'z']\n",
    "    \n",
    "    def __init__(self, width, height, depth, offset):\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.depth = depth\n",
    "        \n",
    "        # We define offset as the depth distance between image plane and \n",
    "        # the ground plane\n",
    "        self.offset = offset\n",
    "        \n",
    "        self.x = width//2\n",
    "        self.y = height//2\n",
    "        self.z = 0\n",
    "        \n",
    "        detection_plane = _orderPoints_()\n",
    "        \n",
    "        self.ROI = None\n",
    "        \n",
    "        self.objects =  orderedDict()\n",
    "        \n",
    "        #tracker = cv2.MultiTracker_create('CSRT')\n",
    "        \n",
    "    def get_object(self, obj):\n",
    "        #self.objects['obj'] = \n",
    "        \n",
    "        return \n",
    "    \n",
    "    def get_HG(self):\n",
    "        class_plane = np.array([[0,0],\n",
    "                        [self.width-1, 0],\n",
    "                        [self.width-1, self.depth-self.offset-1],\n",
    "                        [0, self.depth-self.offset-1]],dtype='int32')\n",
    "        H, _ = cv2.findHomography(self.detection_plane, \n",
    "                            class_plane, method=0, \n",
    "                            ransacReprojThreshold=3.0, \n",
    "                            status=None)\n",
    "        return H\n",
    "        \n",
    "    def addBoard(self, board_obj, position):\n",
    "        if len(position) == 2:\n",
    "            self.ROI = (position[0] + board_obj.cx, position[1]+ board_obj.cy)\n",
    "        else:\n",
    "            self.ROI = (self.x, self.y)\n",
    "            \n",
    "        print('{} Board added at {}'.format(board_obj, self.ROI))\n",
    "        #self.objects['board_obj'] = (position[0], position[1], self.Point.z)\n",
    "        return self.ROI\n",
    "    \n",
    "class Board:\n",
    "    def __init__(self, width, height):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.depth = 0\n",
    "        self.cx, self.cy = width//2, height//2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9e4e5c13fac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9e4e5c13fac0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcsci595\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassroom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-da38450cd3d4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, depth, offset)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdetection_plane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_orderPoints_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8bc3142cbdd8>\u001b[0m in \u001b[0;36m_orderPoints_\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mOP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mOP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mOP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbottonw_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    c_w, c_h, c_d, offset = (400, 150, 300, 50)\n",
    "    b_w, b_h = (200,100)\n",
    "    \n",
    "    csci595 = Classroom(c_w, c_h, c_d, offset)\n",
    "    board = Board(b_w, b_h)\n",
    "    \n",
    "    position = ((c_w-b_w)//2,(c_h-b_h)//2)\n",
    "    \n",
    "    csci595.addBoard(board, position)\n",
    "    \n",
    "    csci595.HG()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "if __name__ ==main():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-f76bfaca5ddf>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-f76bfaca5ddf>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    self.face_gaze =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Face:\n",
    "    def __init__(self):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "    \n",
    "        #X,Y,Z = getHomography(, Z=0) #move to classroom class\n",
    "        self.face_gaze = \n",
    "        Gaze_total = \n",
    "        \n",
    "    def getPose(self):\n",
    "        #model_f.run()\n",
    "        return theta_x, theta_y, theta_z\n",
    "    \n",
    "    def show_attr(self):\n",
    "        print(self.x, self.y, self.w, self.h, theta_x, theta_y)\n",
    "\n",
    "#We create our eye class which contains eye properties\n",
    "class Eyes(Face):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        eye_aspect_ratio = EAR(self)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        theta_x, theta_y = getPose(self)\n",
    "        \n",
    "    def getPose(self):\n",
    "        #model_e.run(img[])\n",
    "        return theta_x, theta_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create our 3D environment using pygame to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'color' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-48edc9cd810b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUIT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"for x, y, z in verts:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'color' is not defined"
     ]
    }
   ],
   "source": [
    "import pygame, sys, math\n",
    "\n",
    "#Here, we initialize a pygame environment to compute parameters for our game loop.\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "W,H = 300,300;\n",
    "cx,cy = W//2,H//2\n",
    "screen = pygame.display.set_mode((W, H))\n",
    "w,h = 100,100\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "verts = (-1,-1,-1),(1,-1,-1),(-1,1,-1),(-1,-1,1),(1,1,-1),(1,-1,1),(-1,1,1),(1,1,1)\n",
    "edges = (0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4),(0,4),(3,7),(2,6),(1,5)\n",
    "#img = \"../data/pygame_ball.jpeg\"\n",
    "#ball = pygame.image.load(img)\n",
    "#ballrect = ball.get_rect()\n",
    "\n",
    "while True :\n",
    "    dt = clock.tick()/100\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT: False\n",
    "    screen.fill((255,255,255))\n",
    "    pygame.draw.rect(screen, color, (cx,cy, w, h))\n",
    "    pygame.display.update()\n",
    "    \"\"\"for x, y, z in verts:\n",
    "        z+=5\n",
    "        f = 200/z\n",
    "        x,y = x*f,y*f\n",
    "        pygame.draw.circle(screen,(0,0,0),(cx+int(x),cy+int(y)),5)\n",
    "        \n",
    "    for edge in edges:\n",
    "        points = []\n",
    "        for x,y,z in (verts[edge[0]],verts[edge[1]]):\n",
    "            z+=5\n",
    "            f = 200/z\n",
    "            x,y = x*f, y*f\n",
    "            points+=[cx+int(x), cy+int(y)] \n",
    "        pygame.draw.circle(screen,(0,0,0), points[0],points[1],1)\"\"\"\n",
    "    \n",
    "    pygame.display.flip()\n",
    "pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):vc\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.Tracker)\n",
    "tracker = cv2.Tracker(img, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate frontal face detector and predictor\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../iris-tracker-by-contours/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#Using dlib face shape predictor, we extract the index for the left and right eyes \n",
    "lEye_l,lEye_r = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "rEye_l,rEye_r  = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell runs on every frame of the stream which runs an \n",
    "# object tracker and a face detector at next frame under \n",
    "# certain conditions and constrainst as described below.\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    \n",
    "    _,frame = cam.read()\n",
    "    roi = frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect dlib face rectangles\n",
    "    faces = face_detector(gray, 0)\n",
    "    \n",
    "    for k, face in enumerate(faces):\n",
    "        \n",
    "        eyes = []\n",
    "        \n",
    "        # convert dlib rect to a bounding box\n",
    "        x,y,w,h = face_utils.rect_to_bb(face)\n",
    "        \n",
    "        # print(x,y,w,h)\n",
    "        xc,yc = int(x+w/2),int(y+h/2)\n",
    "        radius = int((w**2+h**2)**0.5)\n",
    "        \n",
    "        cv2.circle(frame,(xc,yc), radius, (255,0,0), thickness=1)\n",
    "        \n",
    "        # We c \n",
    "        \n",
    "        _face = predictor(gray, face)\n",
    "        _face = face_utils.shape_to_np(_face)\n",
    "        print(_face)\n",
    "        \n",
    "        # Eye indexes\n",
    "        leftEye = _face[lEye_l:lEye_r]\n",
    "        #leftEye_mid = LA.norm(eye[0], eye[3],1)\n",
    "        #rightEye_mid = LA.norm(eye[0], eye[3],1)\n",
    "        rightEye = _face[rEye_l:rEye_r]\n",
    "\n",
    "        Eye1 = Eye(leftEye)\n",
    "        Eye2 = Eye(rightEye)\n",
    "        \n",
    "        eyes.append(Eye1)  #wrap in a list\n",
    "        eyes.append(Eye2)\n",
    "        \n",
    "\n",
    "        # loop through both eyes\n",
    "        for index, eye in enumerate(eyes):\n",
    "\n",
    "            eye_EAR = Ear(eye)\n",
    "            single_eye_state = []  # first entry is eye index, second entry is the closed or eye direction state\n",
    "\n",
    "            left_side_eye = eye[0]  # left edge of eye\n",
    "            right_side_eye = eye[3]  # right edge of eye\n",
    "            top_side_eye = eye[1]  # top side of eye\n",
    "            bottom_side_eye = eye[4]  # bottom side of eye\n",
    "\n",
    "            # calculate height and width of dlib eye keypoints\n",
    "            eye_width = right_side_eye[0] - left_side_eye[0]\n",
    "            eye_height = bottom_side_eye[1] - top_side_eye[1]\n",
    "\n",
    "            # create bounding box with buffer around keypoints\n",
    "            eye_x1 = int(left_side_eye[0] - 0 * eye_width)  # .25 works well too\n",
    "            eye_x2 = int(right_side_eye[0] + 0 * eye_height)  # .75 works well too\n",
    "\n",
    "            eye_y1 = int(top_side_eye[1] - 1 * eye_height)\n",
    "            eye_y2 = int(bottom_side_eye[1] + 1 * eye_height)\n",
    "\n",
    "            # draw bounding box around eye roi\n",
    "            cv2.rectangle(frame,(eye_x1, eye_y1), (eye_x2, eye_y2),(0,255,0),2)\n",
    "\n",
    "            # draw the circles for the eye landmarks\n",
    "            for i in eye:\n",
    "                cv2.circle(frame, tuple(i), 3, (0, 0, 255), -1)\n",
    "                \n",
    "            # d=10920.0/float(w)\n",
    "\n",
    "            roi = frame[eye_y1:eye_y2,eye_x1:eye_x2]\n",
    "\n",
    "            #  ---------    check if eyes open   -------------  #\n",
    "\n",
    "            # state is open/close, or direction looking\n",
    "            eye_state = None\n",
    "\n",
    "            if eye_EAR > 0.25:\n",
    "\n",
    "                #  ---------    find center of pupil   -------------  #\n",
    "\n",
    "                gray=cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)  # grey scale convert\n",
    "                blur = cv2.medianBlur(gray,5) # blue image to find the iris better\n",
    "                equ = cv2.equalizeHist(blur)  # ie, improve contrast by spreading the range over the same window of intensity\n",
    "                thres=cv2.inRange(equ,0,15)  # threshold the contour edges, higher number means more will be black\n",
    "                kernel = np.ones((3,3),np.uint8)  # placeholder\n",
    "\n",
    "            #     #/------- removing small noise inside the white image ---------/#\n",
    "                dilation = cv2.dilate(thres,kernel,iterations = 2)\n",
    "            #     #/------- decreasing the size of the white region -------------/#\n",
    "                erosion = cv2.erode(dilation,kernel,iterations = 3)\n",
    "            #     #/-------- finding the contours -------------------------------/#\n",
    "                image, contours, hierarchy = cv2.findContours(erosion,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "            #     #--------- checking for 2 contours found or not ----------------#\n",
    "\n",
    "                pupil_found = None\n",
    "\n",
    "                if len(contours)==2 :\n",
    "                    # print('2 contours found')\n",
    "                    pupil_found = True\n",
    "\n",
    "                    img = cv2.drawContours(roi, contours, 1, (0,255,0), 3)\n",
    "                    #------ finding the centroid of the contour ----------------#\n",
    "                    M = cv2.moments(contours[1])\n",
    "\n",
    "                    if M['m00']!=0:\n",
    "                        cx = int(M['m10']/M['m00'])\n",
    "                        cy = int(M['m01']/M['m00'])\n",
    "                        cv2.line(roi,(cx,cy),(cx,cy),(0,0,255),3)\n",
    "                        # print(cx,cy)\n",
    "                        \n",
    "                #we\n",
    "\n",
    "                if len(contours)==1:\n",
    "                    pupil_found = True\n",
    "                    # print('only 1 contour found ------- ')\n",
    "\n",
    "                    img = cv2.drawContours(roi, contours, 0, (0,255,0), 3)\n",
    "\n",
    "                    #------- finding centroid of the contour ----#\n",
    "                    M = cv2.moments(contours[0])\n",
    "                    if M['m00']!=0:\n",
    "                        cx = int(M['m10']/M['m00'])\n",
    "                        cy = int(M['m01']/M['m00'])\n",
    "                        # print(cx,cy)\n",
    "                        cv2.line(roi,(cx,cy),(cx,cy),(0,0,255),3)\n",
    "\n",
    "                if pupil_found:\n",
    "                    # find ratio of distance from each side of the eye bounding box\n",
    "                    # to get quantify direction of pupil\n",
    "\n",
    "                    width_ratio = cx / eye_width\n",
    "                    height_ratio = cy / (eye_y2 - eye_y1)  # make sure to use bounding box height\n",
    "\n",
    "                # eyes are opened, but pupils not found\n",
    "                else:\n",
    "                    print('Pupil not found')\n",
    "                    single_eye_state.append(index)\n",
    "                    single_eye_state.append('No pupil found')\n",
    "\n",
    "\n",
    "        # end loop for one face\n",
    "\n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "vs.release()\n",
    "# print(\"accurracy=\",(float(numerator)/float(numerator+denominator))*100)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Gaze vector estimation and visualization\n",
    "\n",
    "This notebook is a reference to _Paper: gaze estimation using a camera-based model in a classroom_ for running preliminary analysis on a webcam.\n",
    "To run this notebook, a built-in webcam and good lighting is required. Meanwhile, some operating systems may require additional firewall settings to gain camera access. We make use of opencv's library to perform video processing and analysis for prelimninary tests on gaze capture.\n",
    "\n",
    "Our gaze estimation application can be sectioned into:\n",
    " - Data pre-processing (face and eye detection)\n",
    " - Head pose classifiers (Euler angles: 𝛼,𝜃,𝜔) or reference plane\n",
    " - Attention boundary/matrix\n",
    " - Depth estimation function\n",
    " - Composition and visualization of gaze vector frequencies\n",
    "Here, our model is primarily based on the relibility of our pre-trained face/eye detection(haarcascades). This dependency may undermine overall perfomance at scale. We aim to detect and identify multiple candidates in each frame of the image in order to estimate head and iris orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
